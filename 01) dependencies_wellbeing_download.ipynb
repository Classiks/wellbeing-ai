{"cells":[{"cell_type":"markdown","metadata":{"id":"HvRBtgL5RC9G"},"source":["# Download and prepare NGram Data"]},{"cell_type":"markdown","metadata":{"id":"yr4Jurk5RC9I"},"source":["!<br>To use Googles fast Internet, upload the notebook to google colab and run the cell below to connect google drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40349,"status":"ok","timestamp":1641549934789,"user":{"displayName":"Daniel Schulz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAC_WPiq6ujOLuyacC0T2DjNwBUqzNCJEpPUJM-w=s64","userId":"14228233649335373663"},"user_tz":-60},"id":"zbHTtvSGRHY1","outputId":"88cec5c8-afcc-458f-9516-5ce576b3fc5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["import googleapiclient\n","import pydrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from google.colab import drive\n","from oauth2client.client import GoogleCredentials\n","from os import chdir\n","\n","TARGET_DIR = 'PATH'\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","my_drive = GoogleDrive(gauth)\n","drive.mount('/content/gdrive')\n","chdir(TARGET_DIR)"]},{"cell_type":"markdown","metadata":{"id":"uD3o9LdPRC9L"},"source":["## Load packages"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":377,"status":"ok","timestamp":1641549935153,"user":{"displayName":"Daniel Schulz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAC_WPiq6ujOLuyacC0T2DjNwBUqzNCJEpPUJM-w=s64","userId":"14228233649335373663"},"user_tz":-60},"id":"bhGVHKX9RC9M"},"outputs":[],"source":["from tqdm.notebook import tqdm # for ipynb\n","import urllib.request\n","import pandas as pd\n","import gzip\n","import os\n","import pickle\n","import re\n","from bs4 import BeautifulSoup\n","from datetime import datetime\n","import gc\n","import multiprocessing as mp"]},{"cell_type":"markdown","metadata":{"id":"DDuX-IO_RC9W"},"source":["## Define helper functions"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1641549935155,"user":{"displayName":"Daniel Schulz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAC_WPiq6ujOLuyacC0T2DjNwBUqzNCJEpPUJM-w=s64","userId":"14228233649335373663"},"user_tz":-60},"id":"CBCr9WL7RC9X"},"outputs":[],"source":["def save_obj(obj, name):  # save python objects\n","    with open(name, 'wb') as f:\n","        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n"," \n"," \n","def load_obj(name):  # load python objects\n","    with open(name, 'rb') as f:\n","        return pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"fnbtO0uMRC9e"},"source":["## Set language and words to extract"]},{"cell_type":"markdown","metadata":{"id":"IH5gLHqrRC9n"},"source":["### Define Pattern to extract"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1641549935156,"user":{"displayName":"Daniel Schulz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAC_WPiq6ujOLuyacC0T2DjNwBUqzNCJEpPUJM-w=s64","userId":"14228233649335373663"},"user_tz":-60},"id":"u2jHbe8rRC9p"},"outputs":[],"source":["import re\n","p_target = re.compile(r'wellbeing_noun=>\\w+_adj')"]},{"cell_type":"markdown","metadata":{"id":"cf-i5hhaRC96"},"source":["## Get NGram Links"]},{"cell_type":"markdown","metadata":{"id":"LsUxMPCZRC97"},"source":["Get Links<br>\n","<b>Set Language Here</b>"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1641550144927,"user":{"displayName":"Daniel Schulz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAC_WPiq6ujOLuyacC0T2DjNwBUqzNCJEpPUJM-w=s64","userId":"14228233649335373663"},"user_tz":-60},"id":"j2qPOfJBRC9_","outputId":"8b3805e7-05ca-490f-bbac-bf211798a855"},"outputs":[{"name":"stdout","output_type":"stream","text":["weather_VERB=>gap - http://storage.googleapis.com/books/ngrams/books/20200217/eng/0-00841-of-00857.gz\n","1\n"]}],"source":["# Set subcorpus here:\n","# change tag after \"books/20200217/\"\n","# Twice for the url, once for the pattern\n","# eng or eng-fiction or eng-us\n","list_url = f'http://storage.googleapis.com/books/ngrams/books/20200217/eng/eng-0-ngrams_exports.html'  # url to list with links\n","p_gzfile = re.compile(r'http://storage\\.googleapis\\.com/books/ngrams/books/20200217/eng/0-00841-of-00857\\.gz')  # pattern for links to gzip files\n","\n","# Downloads all links to the gzip files with starting ngram\n","soup = BeautifulSoup(urllib.request.urlopen(list_url).read(), 'html.parser')\n","links = [(a.text, a['href']) for a in soup.find_all('a') if p_gzfile.search(a['href'])]\n","\n","print('\\n'.join([' - '.join(link) for link in links[:5]]))\n","print(len(links))"]},{"cell_type":"markdown","metadata":{"id":"fwO5ka24RC-N"},"source":["# Download and Extraction"]},{"cell_type":"markdown","metadata":{"id":"4QMa-3WgRC-O"},"source":["## Function"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":266,"status":"ok","timestamp":1641550149828,"user":{"displayName":"Daniel Schulz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAC_WPiq6ujOLuyacC0T2DjNwBUqzNCJEpPUJM-w=s64","userId":"14228233649335373663"},"user_tz":-60},"id":"-rUTqqFeRC-P"},"outputs":[],"source":["def download_extract(process_count, links):\n","    \"\"\"\n","    This function downloads the files needed for the analysis and extracts the data\n","    \"\"\"\n","\n","    print(f'Process {process_count} started...')\n","\n","    amt_files_to_dl = len(links)\n","    downloaded = 0\n","    # lines_parsed = 0\n","\n","    \n","    # load csv with ngram counts if it already exists, otherwise create empty\n","    if os.path.isfile(f'ngram_checkpoint_Process{process_count}.csv'):\n","        ngram_df = pd.read_csv(f'ngram_checkpoint_Process{process_count}.csv')\n","    else:\n","        ngram_df = pd.DataFrame({\n","        'ngram': pd.Series([], dtype='str'),\n","        'year': pd.Series([], dtype='int'),\n","        'count': pd.Series([], dtype='int')\n","      })\n","    \n","    if os.path.isfile(f'completed_files_Process{process_count}.pkl'):\n","        completed_files = load_obj(f'completed_files_Process{process_count}.pkl')\n","    else:\n","        completed_files = []\n","    \n","    # Loop over links\n","    # start = datetime.now()\n","    for entry in links:\n","        link = entry[1]\n","        ngramfile = link.split('/')[-1]\n","        downloaded += 1\n","        linkstart = datetime.now()\n","\n","        if ngramfile in completed_files:\n","        #   print(f'Process {process_count}: File {ngramfile} already downloaded and extracted\\nContinuing...\\n')\n","          continue\n","\n","        # Get file size and show progress\n","        # site = urllib.request.urlopen(link)\n","        # size = site.info()[\"Content-Length\"]\n","        # del site\n","        # gc.collect()\n","        # mbsize = round(int(size)/1000000, 2)\n","        # print(f'File: \"{ngramfile}\" (No. {downloaded}/{amt_files_to_dl}; Size: ~{mbsize} MB)')\n","        print(f'Process {process_count}: File: \"{ngramfile}\" (No. {downloaded}/{amt_files_to_dl})')\n","\n","        # Download file\n","        # print('\\tDownloading ...')\n","        urllib.request.urlretrieve(link, ngramfile)\n","\n","        # Extract information\n","        # print('\\tExtracting ...')\n","\n","        # if any([word == ngram for ngram in ngrams]): pass\n","        # open ngram file (gzip) \n","        relevant_entries = []\n","        with gzip.open(ngramfile, mode='rb') as infile:\n","            # lines_in_file = sum([1 for line in infile.readlines()])\n","            # infile.seek(0)  # Reset readlines() method\n","            # lines_in_file = 3000000  # estimation, more like 3.5e6\n","            for line in infile.readlines():\n","                splitline = line.decode('utf-8').split('\\t')\n","                ngram = splitline[0].lower()\n","                if p_target.search(ngram):  # None converts to false, match to true\n","                  for year in splitline[1:]:\n","                    relevant_entries.append([ngram, *map(int, year.split(',')[:2])])\n","                    # ngram_df.loc[len(ngram_df)] = [ngram, *map(int, year.split(',')[:2])]\n","        new_ngrams = pd.DataFrame.from_records(relevant_entries, columns=[\"ngram\", \"year\", \"count\"])\n","        ngram_df = pd.concat([ngram_df, new_ngrams])\n","        ngram_df.to_csv(f'ngram_checkpoint_Process{process_count}.csv', index=False)\n","        # lines_parsed += lines_in_file + 4e5  # lower number for tqdm, 3.4e6 is closer\n","        print(f'Process {process_count}: {ngramfile} finished in {datetime.now() - linkstart}')\n","        # print(f'\\tTime for file: {datetime.now() - linkstart}')\n","        # print(f'\\tAbout {lines_parsed} lines parsed in {datetime.now() - start}\\n\\n')\n","        # print(f'\\tTotal Time: {datetime.now() - start}\\n\\n')\n","\n","    \n","        completed_files.append(ngramfile)\n","        save_obj(completed_files, f'completed_files_Process{process_count}.pkl')\n","        os.remove(ngramfile)  # remove downloaded file\n","        gc.collect()\n","\n","        # Careful: Removes all files in trash (f\"title = '{ngramfile}' and trashed=true\")\n","        viable_errors = (FileNotFoundError, googleapiclient.errors.HttpError, pydrive.files.ApiRequestError, pydrive.settings.InvalidConfigError)\n","        for file_to_remove in my_drive.ListFile({'q': \"trashed=true\"}).GetList():\n","          try:\n","            file_to_remove.Delete()\n","          except viable_errors as e:\n","            print(f\"Error in Removing file {ngramfile} in Process {process_count}\")\n","            try:\n","              file_to_remove.Delete()\n","            except viable_errors as e:\n","              pass"]},{"cell_type":"markdown","metadata":{"id":"FsejPn9DRC-U"},"source":["## Run Download and extraction"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59711,"status":"ok","timestamp":1641550212070,"user":{"displayName":"Daniel Schulz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAC_WPiq6ujOLuyacC0T2DjNwBUqzNCJEpPUJM-w=s64","userId":"14228233649335373663"},"user_tz":-60},"id":"wSIDoRP3RC-V","outputId":"36da22ce-305d-46ce-c370-f04b80de8179"},"outputs":[{"name":"stdout","output_type":"stream","text":["Process 1 started...\n","Process 2 started...\n","Process 3 started...\n","Process 4 started...\n","Process 5 started...\n","Process 6 started...\n","Process 6: File: \"0-00841-of-00857.gz\" (No. 1/1)\n","Process 6: 0-00841-of-00857.gz finished in 0:00:41.397126\n","______________________________\n","\n","DONE\n"]}],"source":["# download_extract(assigned)\n","NUM_PROCESSES = 6\n"," \n","share, rest = divmod(len(links), NUM_PROCESSES)\n","ngram_shares = []\n","for i in range(0, NUM_PROCESSES-1):\n","  ngram_shares.append(links[share*i:share*(i+1)])\n","ngram_shares.append(links[-(share+rest):])\n","\n","processes = []\n","for i, share in enumerate(ngram_shares):\n","  processes.append(mp.Process(target=download_extract, args=(i+1, share, )))\n"," \n","for process in processes:\n","  process.start()\n"," \n","for process in processes:\n","  process.join()\n"," \n","print('_'*30 + '\\n\\nDONE')"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"01) x lead(s) to wellbeing.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"0f2f6c680cf484319d387fabac80ca4ff4fc33965036e02100b3fe02600f1423"}}},"nbformat":4,"nbformat_minor":0}
